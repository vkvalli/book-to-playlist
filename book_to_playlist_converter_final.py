# -*- coding: utf-8 -*-
"""book_to_playlist_converter-final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11X5bKWbPzZESrXlXt6y8jE1YkjY-F0MR

# Imports and setup
"""

from google.colab import files
files.upload()  # This will prompt you to upload the JSON file

import os
os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = "/content/xenon-coast-443204-b6-000a00c6a090.json"

!pip install google-cloud-storage
from google.colab import auth
# Install PySpark and Hadoop GCS dependencies
!apt-get install openjdk-8-jdk-headless -qq > /dev/null
!pip install pyspark

from google.cloud import storage

# Initialize a GCS client
client = storage.Client()

# Replace with your GCS bucket name and file path
bucket_name = 'bookandmusicdatasets'
file1 = 'BooksDataSet.csv'
file2 = 'book_details.csv'
file3 = 'books_2.csv'
file4 = 'spotify_tracks.csv'
file5 = 'lyrics_features.csv'

# Access the file from GCS
bucket = client.get_bucket(bucket_name)
blob1 = bucket.blob(file1)
blob2 = bucket.blob(file2)
blob3 = bucket.blob(file3)
blob4 = bucket.blob(file4)
blob5 = bucket.blob(file5)

# Download the file locally in Colab
blob1.download_to_filename('/content/BooksDataSet.csv')
blob2.download_to_filename('/content/book_details.csv')
blob3.download_to_filename('/content/books_2.csv')
blob4.download_to_filename('/content/spotify_tracks.csv')
blob5.download_to_filename('/content/lyrics_features.csv')

"""# Book dataset preprocessing"""

import pandas as pd
dfbds = pd.read_csv('/content/BooksDataSet.csv')
dfbd = pd.read_csv('/content/book_details.csv')
dfbk = pd.read_csv('/content/books_2.csv')

dfbds.head()
dfbd.head()
dfbk.head()

dfbds.info()
dfbd.info()
dfbk.info()

dfbds = dfbds[['book_name', 'genre', 'summary']]
dfbk = dfbk[['Book-Title', 'Book-Author', 'description']]

dfbds.info()
dfbd.info()
dfbk.info()

import pandas as pd

# Dataset 1: Rename and add missing columns
dfbds.rename(columns={'book_name': 'title', 'genre': 'genre', 'summary': 'summary'}, inplace=True)
dfbds['author'] = None  # Add missing 'author' column with null values

# Dataset 2: Rename and add missing columns
dfbd.rename(columns={'title': 'title', 'description': 'summary', 'genres': 'genre'}, inplace=True)
dfbd['author'] = None  # Add missing 'author' column with null values

# Dataset 3: Rename and add missing columns
dfbk.rename(columns={'Book-Title': 'title', 'Book-Author': 'author', 'description': 'summary'}, inplace=True)
dfbk['genre'] = None  # Add missing 'genre' column with null values

# Combine 2 datasets into one
combined_df = pd.concat([dfbds, dfbd], ignore_index=True)

# Reorder the columns for consistency
combined_df = combined_df[['title', 'author', 'summary', 'genre']]

# Fill missing values
combined_df['author'].fillna('unknown', inplace=True)

# Display the combined dataset
print(combined_df.info())
print(combined_df.head())
print(dfbk.info())
print(dfbk.head())

import re

# Function to clean text
def clean_text(text):
    text = re.sub(r'[^\w\s]', '', text)  # Remove special characters
    text = text.lower()  # Convert to lowercase
    text = re.sub(r'\s+', ' ', text)  # Remove extra whitespace
    return text

# Apply cleaning to relevant columns
combined_df['title'] = combined_df['title'].apply(clean_text)
combined_df['summary'] = combined_df['summary'].apply(clean_text)
combined_df['genre'] = combined_df['genre'].apply(clean_text)

combined_df['summary_length'] = combined_df['summary'].apply(len)
combined_df['summary_word_count'] = combined_df['summary'].apply(lambda x: len(x.split()))

# Example of label encoding for genres
from sklearn.preprocessing import LabelEncoder

le = LabelEncoder()
combined_df['genre_encoded'] = le.fit_transform(combined_df['genre'])

import os
import pandas as pd
import numpy as np
from sentence_transformers import SentenceTransformer
from google.cloud import storage

# Load pre-trained BERT-based Sentence Transformer
model = SentenceTransformer('all-MiniLM-L6-v2')

# Initialize GCS client
client = storage.Client()

# Function to upload a file to GCS
def upload_to_gcs(local_file_path, bucket_name, destination_blob_name):
    bucket = client.bucket(bucket_name)
    blob = bucket.blob(destination_blob_name)
    blob.upload_from_filename(local_file_path)
    print(f"Uploaded {local_file_path} to {bucket_name}/{destination_blob_name}.")

# Function to download a file from GCS
def download_from_gcs(bucket_name, source_blob_name, local_file_path):
    bucket = client.bucket(bucket_name)
    blob = bucket.blob(source_blob_name)
    try:
        blob.download_to_filename(local_file_path)
        print(f"Downloaded {source_blob_name} to {local_file_path}.")
        return True
    except Exception as e:
        print(f"File {source_blob_name} does not exist in GCS. Error: {e}")
        return False

# Function to list existing batches in GCS
def list_gcs_files(bucket_name, prefix):
    bucket = client.bucket(bucket_name)
    blobs = bucket.list_blobs(prefix=prefix)
    return [blob.name for blob in blobs]

# Function to load previously saved embeddings or process and save batches
def load_or_encode_batches(sentences, model, batch_size=32, output_dir="embeddings_batches", bucket_name=None):
    embeddings = []
    total_batches = len(sentences) // batch_size + int(len(sentences) % batch_size != 0)
    for i in range(0, len(sentences), batch_size):
        batch_index = i // batch_size
        local_file_path = os.path.join(output_dir, f"batch_{batch_index}.npy")
        gcs_blob_name = f"bookandmusicdatasets/summary_embeddings/batch_{batch_index}.npy"

        # Check if batch exists locally or in GCS
        if os.path.exists(local_file_path) or (bucket_name and download_from_gcs(bucket_name, gcs_blob_name, local_file_path)):
            print(f"Batch {batch_index} already processed. Loading...")
            batch_embeddings = np.load(local_file_path)
        else:
            # Process and save batch locally and to GCS
            print(f"Processing batch {batch_index + 1}/{total_batches}...")
            batch = sentences[i:i + batch_size]
            batch_embeddings = model.encode(batch, show_progress_bar=True)
            np.save(local_file_path, batch_embeddings)
            if bucket_name:
                upload_to_gcs(local_file_path, bucket_name, gcs_blob_name)

        embeddings.append(batch_embeddings)
    return np.vstack(embeddings)

# Ensure local directory exists
os.makedirs("embeddings_batches", exist_ok=True)

# Process summaries and load embeddings
gcs_prefix = "bookandmusicdatasets/summary_embeddings/"
summaries = combined_df['summary'].tolist()
gcs_existing_batches = list_gcs_files(bucket_name, gcs_prefix)

# Identify missing batches
total_batches = len(summaries) // 32 + int(len(summaries) % 32 != 0)
missing_batches = [
    batch_index for batch_index in range(total_batches)
    if f"{gcs_prefix}batch_{batch_index}.npy" not in gcs_existing_batches
]

# Process missing batches
for batch_index in missing_batches:
    start_idx = batch_index * 32
    end_idx = start_idx + 32
    batch = summaries[start_idx:end_idx]
    local_file_path = f"embeddings_batches/batch_{batch_index}.npy"
    gcs_blob_name = f"{gcs_prefix}batch_{batch_index}.npy"
    print(f"Processing missing batch {batch_index}...")
    batch_embeddings = model.encode(batch, show_progress_bar=True)
    np.save(local_file_path, batch_embeddings)
    upload_to_gcs(local_file_path, bucket_name, gcs_blob_name)
    print(f"Batch {batch_index} uploaded to GCS.")

# Load or reprocess all batches
summary_embeddings = load_or_encode_batches(summaries, model, batch_size=32, output_dir="embeddings_batches", bucket_name=bucket_name)

# Ensure the embeddings are in the correct shape
summary_embeddings_df = pd.DataFrame(summary_embeddings)

# Combine the embeddings with the original DataFrame
combined_df_features = pd.concat(
    [combined_df.drop(columns=['summary']), summary_embeddings_df], axis=1
)

# Inspect the new feature DataFrame
print(combined_df_features.head())

# Prepare training data
X_train = combined_df['summary']
y_train = combined_df['genre']

import os
import torch
import torch.nn as nn
import torch.optim as optim
from sentence_transformers import SentenceTransformer
import numpy as np
from sklearn.preprocessing import StandardScaler
from torch.utils.data import DataLoader, TensorDataset
from google.cloud import storage

# Directory to save intermediate embeddings locally
output_dir = "X_train_embeddings_batches"
os.makedirs(output_dir, exist_ok=True)

# Authenticate and set up GCS
client = storage.Client()
bucket_name = "bookandmusicdatasets"  # Replace with your GCS bucket name
bucket = client.bucket(bucket_name)

# Function to upload a file to GCS
def upload_to_gcs(local_file_path, bucket_name, destination_blob_name):
    blob = bucket.blob(destination_blob_name)
    blob.upload_from_filename(local_file_path)
    print(f"Uploaded {local_file_path} to {bucket_name}/{destination_blob_name}.")

# Function to download a file from GCS
def download_from_gcs(bucket_name, source_blob_name, local_file_path):
    try:
        blob = bucket.blob(source_blob_name)
        blob.download_to_filename(local_file_path)
        print(f"Downloaded {source_blob_name} to {local_file_path}.")
        return True
    except Exception as e:
        print(f"File {source_blob_name} does not exist in GCS. Error: {e}")
        return False

# Function to batch process embeddings with intermediate saving
def batch_encode_and_save(data, model, batch_size=32, output_dir="X_train_embeddings_batches"):
    embeddings = []
    total_batches = len(data) // batch_size + int(len(data) % batch_size != 0)

    for i in range(0, len(data), batch_size):
        batch_index = i // batch_size
        batch_file = os.path.join(output_dir, f"batch_{batch_index}.npy")
        gcs_blob_name = f"X_train_embeddings/batch_{batch_index}.npy"

        # Check if batch exists locally or in GCS
        if os.path.exists(batch_file) or download_from_gcs(bucket_name, gcs_blob_name, batch_file):
            print(f"Batch {batch_index} already processed. Loading...")
            batch_embeddings = np.load(batch_file)
        else:
            print(f"Processing batch {batch_index + 1}/{total_batches}...")
            batch = data[i:i + batch_size]
            batch_embeddings = model.encode(batch, show_progress_bar=False)
            np.save(batch_file, batch_embeddings)
            upload_to_gcs(batch_file, bucket_name, gcs_blob_name)

        embeddings.append(batch_embeddings)

    return np.vstack(embeddings)

# Load pre-trained BERT embeddings model
bert_model = SentenceTransformer('all-MiniLM-L6-v2')

# Generate embeddings for X_train using BERT with intermediate saving
X_train_embeddings = batch_encode_and_save(X_train.tolist(), bert_model, batch_size=32, output_dir=output_dir)

# Normalize embeddings to ensure uniform scale
scaler = StandardScaler()
X_train_embeddings_normalized = scaler.fit_transform(X_train_embeddings)

# Ensure no NaN or Infinite values in the embeddings
if np.any(np.isnan(X_train_embeddings_normalized)) or np.any(np.isinf(X_train_embeddings_normalized)):
    print("NaN or Infinite values detected in X_train_embeddings!")
else:
    print("No NaN or Infinite values detected in X_train_embeddings.")

# Ensure that y_train is correctly encoded (as integer classes)
y_train = combined_df['genre_encoded']

# Convert data to torch tensors
X_train_tensor = torch.tensor(X_train_embeddings_normalized, dtype=torch.float32)
y_train_tensor = torch.tensor(y_train.values, dtype=torch.long)

# Create a dataset and DataLoader for mini-batch training
batch_size = 32  # Smaller batch size for CPU training
dataset = TensorDataset(X_train_tensor, y_train_tensor)
dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)

# Multi-Class Classification Model with Hidden Layer and Dropout
class MultiClassModel(nn.Module):
    def __init__(self, input_dim, hidden_dim, num_classes):
        super(MultiClassModel, self).__init__()
        self.fc1 = nn.Linear(input_dim, hidden_dim)
        self.relu = nn.ReLU()
        self.dropout = nn.Dropout(p=0.5)  # Dropout for regularization
        self.fc2 = nn.Linear(hidden_dim, num_classes)

    def forward(self, x):
        x = self.fc1(x)
        x = self.relu(x)
        x = self.dropout(x)
        x = self.fc2(x)
        return x

# Initialize model
input_dim = X_train_tensor.shape[1]
hidden_dim = 128  # Size of the hidden layer
num_classes = len(np.unique(y_train))  # Number of unique classes (genres)
model = MultiClassModel(input_dim, hidden_dim, num_classes)

# Define loss and optimizer with regularization and adjusted learning rate
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-5)  # L2 regularization

# Training the model with mini-batch
num_epochs = 10
for epoch in range(num_epochs):
    model.train()
    running_loss = 0.0

    for batch_idx, (inputs, labels) in enumerate(dataloader):
        # Forward pass
        outputs = model(inputs)

        # Compute the loss
        loss = criterion(outputs, labels)

        # Backward pass and optimization
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        # Accumulate the loss
        running_loss += loss.item()

    # Print average loss for this epoch
    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss / len(dataloader):.4f}')

# Save the trained model for future inference
torch.save(model.state_dict(), "multi_class_model.pth")

# Set device to GPU if available, otherwise fallback to CPU
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# Prepare the text features for prediction
bert_model = SentenceTransformer('all-MiniLM-L6-v2')  # Load the pre-trained SentenceTransformer model
X_dfbk_embeddings = bert_model.encode(dfbk['summary'].tolist(), show_progress_bar=True)

# Normalize embeddings for compatibility with the trained model
scaler = StandardScaler()
X_dfbk_embeddings_normalized = scaler.fit_transform(X_dfbk_embeddings)

# Convert to tensor and move to the same device as the model
X_dfbk_tensor = torch.tensor(X_dfbk_embeddings_normalized, dtype=torch.float32).to(device)

# Load the model state from saved checkpoint
model = MultiClassModel(input_dim=X_dfbk_tensor.shape[1], hidden_dim=128, num_classes=len(np.unique(y_train)))
model.load_state_dict(torch.load("multi_class_model.pth", map_location=device))
model.to(device)

# Set the model to evaluation mode
model.eval()

# Perform prediction
with torch.no_grad():
    # Perform forward pass
    outputs = model(X_dfbk_tensor)
    # Extract the class with the highest probability
    _, predicted_genres = torch.max(outputs, 1)

# Convert predictions back to NumPy array after moving to CPU
predicted_genres = predicted_genres.cpu().numpy()

# Update the DataFrame with the predicted genres
dfbk['predicted_genre'] = predicted_genres

dfbk.info()
combined_df.info()

unified_df = pd.concat([combined_df, dfbk], ignore_index=True)

# Optional: Remove duplicates based on critical columns
unified_df = unified_df.drop_duplicates(subset=['title', 'author', 'summary'], keep='first')

# Inspect the final unified DataFrame
print(unified_df.head())

"""# Songs dataset preprocessing"""

dfs = pd.read_csv('/content/spotify_tracks.csv')
dflf = pd.read_csv('/content/lyrics_features.csv')

dfs.head()
dflf.head()

dfs.info()
dflf.info()

# Step 1: Preprocess the Spotify tracks dataset
def preprocess_spotify_tracks(df):
    # Drop unnecessary columns
    columns_to_drop = [
        'analysis_url', 'artists_id', 'available_markets', 'href', 'preview_url', 'track_href',
        'track_name_prev', 'type', 'album_id', 'disc_number', 'time_signature'
    ]

    df.drop(columns=columns_to_drop, inplace=True, errors='ignore')

    # Rename columns for clarity
    df.rename(columns={
        'name': 'song_title',
        'id': 'track_id'
    }, inplace=True)

    # Drop rows with null values in critical audio features
    df.dropna(subset=['danceability', 'energy', 'instrumentalness', 'loudness', 'tempo', 'popularity'], inplace=True)

    # Reset index
    df.reset_index(drop=True, inplace=True)

    return df

dfs = preprocess_spotify_tracks(dfs)
    # Final steps
print(dfs.head())
print(dfs.info())

# Preprocess lyrics features
def preprocess_lyrics_features(df):
    # Rename for consistency
    df.rename(columns={
        'mean_syllables_word': 'mean_syllables',
        'mean_words_sentence': 'mean_words',
        'n_sentences': 'num_sentences',
        'n_words': 'num_words',
        'sentence_similarity': 'similarity_score'
    }, inplace=True)

    # Ensure all columns have numeric values, handling nulls or type mismatches
    df.fillna(0, inplace=True)

    # Drop rows with null or missing track IDs
    df.dropna(subset=['track_id'], inplace=True)

    # Reset index
    df.reset_index(drop=True, inplace=True)

    return df

# Preprocess both datasets

dflf = preprocess_lyrics_features(dflf)
print(dflf.info())

# Step 3: Optionally merge datasets by 'track_id'
def merge_datasets(dfs, dflf):
    # Perform an outer join on 'track_id'
    merged_df = pd.merge(dfs, dflf, how='left', on='track_id')
    # Handle missing values after merging (e.g., fill NaNs with 0 for numerical features)
    merged_df.fillna(0, inplace=True)
    return merged_df
# If combining is desired
combined_df = merge_datasets(dfs, dflf)

print("\nCombined Dataset Info (Optional):")
print(combined_df.info())

# Drop unnecessary or redundant columns
def clean_combined_dataset(df):
    # Drop index-like columns from the merging process
    columns_to_drop = [
        'Unnamed: 0_x', 'Unnamed: 0_y'
    ]

    # Perform drop safely
    df.drop(columns=columns_to_drop, inplace=True, errors='ignore')

    # Optionally drop any other unnecessary or overly specific columns
    # Keep only the critical columns for modeling
    selected_columns = [
        'acousticness', 'danceability', 'energy', 'instrumentalness',
        'loudness', 'speechiness', 'tempo', 'popularity', 'valence',
        'key', 'liveness','lyrics', 'mode', 'song_title',
        'mean_syllables', 'mean_words', 'similarity_score', 'vocabulary_wealth',
        'num_sentences', 'num_words'
    ]

    # Subset the dataframe
    df = df[selected_columns]

    # Reset index after cleanup
    df.reset_index(drop=True, inplace=True)

    return df


# Clean the combined dataset
combined_dfs_cleaned = clean_combined_dataset(combined_df)

# Inspect final columns
print("Cleaned Combined Dataset Info:")
print(combined_dfs_cleaned.info())

pip install sentence-transformers

# Import necessary libraries
import re
import nltk
from sklearn.feature_extraction.text import TfidfVectorizer
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer

# Ensure stopwords are downloaded
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('punkt_tab')
nltk.download('wordnet')

# Function to clean and preprocess lyrics
def preprocess_lyrics(text):
    # Lowercase the text
    text = text.lower()

    # Remove special characters, numbers, and punctuation
    text = re.sub(r'[^a-z\s]', '', text)  # Remove anything except letters and spaces

    # Tokenize text into words
    tokens = nltk.word_tokenize(text)

    # Remove stopwords
    stop_words = set(stopwords.words('english'))
    tokens = [word for word in tokens if word not in stop_words]

    # Lemmatize words
    lemmatizer = WordNetLemmatizer()
    tokens = [lemmatizer.lemmatize(word) for word in tokens]

    # Join tokens back into a string
    preprocessed_text = ' '.join(tokens)

    return preprocessed_text


# Safely apply preprocessing to the lyrics column
combined_dfs_cleaned.loc[:, 'lyrics_cleaned'] = combined_dfs_cleaned['lyrics'].apply(preprocess_lyrics)

# Inspect the preprocessed text
print(combined_dfs_cleaned['lyrics_cleaned'].head())

unified_df.info()
combined_dfs_cleaned.info()

"""# Building aa recoomendation system"""

# Filter rows where predicted_genre is not null
non_null_predicted = unified_df[unified_df['predicted_genre'].notnull()]

# Compare the values in predicted_genre and genre_encoded
matching = non_null_predicted['predicted_genre'] == non_null_predicted['genre_encoded']

# Check if all values match
if matching.all():
    print("All non-null predicted_genre values match genre_encoded.")
    unified_df.drop(columns=['predicted_genre'], inplace=True)
else:
    print("Some values in predicted_genre do not match genre_encoded.")

def upload_to_gcs(local_file_path, bucket_name, destination_blob_name):

    blob = bucket.blob(destination_blob_name)
    blob.upload_from_filename(local_file_path)
    print(f"Uploaded {local_file_path} to {bucket_name}/{destination_blob_name}.")

# Initialize SentenceTransformer
model = SentenceTransformer('all-MiniLM-L6-v2')

# Define file paths
local_book_path = 'book_summary_embeddings.npy'
gcs_book_path = 'embeddings/book_summary_embeddings.npy'

# Check for existing embeddings
if not os.path.exists(local_book_path):
    print(f"{local_book_path} not found locally. Checking GCS...")
    if download_from_gcs(bucket_name, gcs_book_path, local_book_path):
        print(f"Book summary embeddings downloaded from GCS to {local_book_path}.")
        book_embeddings = np.load(local_book_path)
    else:
        print("Embeddings not found in GCS. Generating new embeddings...")
        # Compute embeddings for book summaries
        print("Computing book summary embeddings...")
        book_embeddings = model.encode(unified_df['summary'].tolist(), show_progress_bar=True)
        # Save locally
        np.save(local_book_path, book_embeddings)
        print(f"Book summary embeddings saved locally to {local_book_path}.")
        # Upload to GCS
        upload_to_gcs(local_book_path, bucket_name, gcs_book_path)
        print(f"Book summary embeddings uploaded to GCS at {gcs_book_path}.")
else:
    print(f"Loading book summary embeddings from local file: {local_book_path}")
    book_embeddings = np.load(local_book_path)

# `book_embeddings` is now ready for use
print(f"Embeddings shape: {book_embeddings.shape}")

import os
import numpy as np

# Check if embeddings already exist locally or in GCS
local_song_path = 'song_lyrics_embeddings.npy'
gcs_song_path = 'embeddings/song_lyrics_embeddings.npy'

# Try downloading from GCS if the local file doesn't exist
if not os.path.exists(local_song_path):
    print(f"{local_song_path} not found locally. Checking GCS...")
    if download_from_gcs(bucket_name, gcs_song_path, local_song_path):
        print(f"Song lyrics embeddings downloaded from GCS to {local_song_path}.")
    else:
        print("Embeddings not found in GCS. Generating new embeddings...")
        # Generate embeddings if not found locally or in GCS
        song_embeddings = model.encode(combined_dfs_cleaned['lyrics_cleaned'].tolist(), show_progress_bar=True)
        np.save(local_song_path, song_embeddings)
        print(f"Song lyrics embeddings saved locally to {local_song_path}.")

        # Upload to GCS
        upload_to_gcs(local_song_path, bucket_name, gcs_song_path)
        print(f"Song lyrics embeddings uploaded to GCS: {gcs_song_path}")
else:
    print(f"Loading song lyrics embeddings from local file: {local_song_path}")
    song_embeddings = np.load(local_song_path)

# `song_embeddings` is now ready to use
print(f"Embeddings shape: {song_embeddings.shape}")

pip install faiss-cpu  # Install for CPU

import numpy as np
from sklearn.decomposition import PCA
from faiss import IndexFlatIP
import pickle
from tqdm import tqdm

# Example initialization (replace with actual data loading)
num_books = 1000
num_songs = 5000
embedding_dim = 512
book_embeddings_tensor = np.random.rand(num_books, embedding_dim).astype(np.float32)
song_embeddings_tensor = np.random.rand(num_songs, embedding_dim).astype(np.float32)

# Preprocess and normalize
pca = PCA(n_components=128)
book_embeddings_tensor = pca.fit_transform(book_embeddings_tensor)
song_embeddings_tensor = pca.transform(song_embeddings_tensor)

book_embeddings_tensor = book_embeddings_tensor / np.linalg.norm(book_embeddings_tensor, axis=1, keepdims=True)
song_embeddings_tensor = song_embeddings_tensor / np.linalg.norm(song_embeddings_tensor, axis=1, keepdims=True)

# Create FAISS index
index = IndexFlatIP(song_embeddings_tensor.shape[1])
index.add(song_embeddings_tensor.astype(np.float32))

# Precompute results
cache_file_path = "similar_songs_cache.pkl"
if os.path.exists(cache_file_path):
    with open(cache_file_path, "rb") as f:
        top_similarities = pickle.load(f)
else:
    top_similarities = []
    for i in tqdm(range(num_books), desc="Precomputing Similarities"):
        D, I = index.search(book_embeddings_tensor[i:i+1].astype(np.float32), k=5)
        top_similarities.append({"book_index": i, "song_indices": I[0].tolist(), "similarities": D[0].tolist()})

    with open(cache_file_path, "wb") as f:
        pickle.dump(top_similarities, f)

# Query example
book_index = 0  # Example book index
results = top_similarities[book_index]
print(f"Top songs for book {book_index}: {results}")

import pandas as pd

# # Example DataFrame to store (assuming unified_df is already created)
# unified_df = pd.read_csv("unified.csv")

# Store the DataFrame as a CSV file locally
unified_df.to_csv("unified_data.csv", index=False)
print("DataFrame saved to 'unified_data.csv'.")

# Optionally, you could verify the file was created:
with open("unified_data.csv", "r") as f:
    print(f.read())  # Print the first few lines of the saved file

# Replace these variables
blob_name = 'unified_data'  # Path in GCS bucket
local_file_path = 'unified_data.csv'

# Authenticate and create a GCS client
storage_client = storage.Client()

# Upload the file
bucket = storage_client.bucket(bucket_name)
blob = bucket.blob(blob_name)
blob.upload_from_filename(local_file_path)

print(f"File uploaded to GCS: {blob_name}")

# unified_df.head()
# print(unified_df.columns)

# # Check unique titles to ensure no leading/trailing spaces
# print(unified_df['title'].unique())

# Sample some rows to inspect
# # print(unified_df['title'].head())

# # Print top_similarities to check its structure
# book_title = input("Enter a book title: ").lower().strip()
# print(f"Book matched: {book_title}")

# # Check if 'title' is the correct column name
# if 'title' not in unified_df.columns:
#     print("Column 'title' not found. Available columns:", unified_df.columns)
#     # You may want to use an alternative column name if necessary
# else:
#     matched_book = unified_df.loc[unified_df['title'].str.contains(book_title, case=False, na=False)]

# if recommended_songs:
#     print("Top recommended songs:")
#     for song in recommended_songs:
#         print(f"- {song['song_title']} by {song['artist']} (Similarity: {song['similarity_score']:.2f})")
# else:
#     print("No recommendations found.")

combined_dfs_cleaned.info()

import pandas as pd

# Load the unified DataFrame
unified_df = pd.read_csv("/content/unified_data.csv")

# Example structure of top_similarities
# top_similarities = {
#     book_index: {
#         'song_indices': [...],
#         'similarities': [...]
#     }
# }

# Function to find similar songs for a given book title
def recommend_songs(book_title, top_k=5):
    book_title = book_title.lower().strip()  # Normalize the book title

    matched_book = unified_df.loc[unified_df['title'].str.lower().str.contains(book_title, na=False)]

    if matched_book.empty:
        print(f"No book found for title: {book_title}")
        return []

    try:
        book_index = matched_book.index[0]  # Use the first match if multiple are found
    except IndexError:
        print(f"No book matched for title: {book_title}")
        return []

    print(f"Book matched: {matched_book.iloc[0]['title']}")

    try:
        song_indices = top_similarities[book_index]['song_indices']
        similarity_scores = top_similarities[book_index]['similarities']
    except KeyError as e:
        print(f"KeyError in top_similarities access: {e}")
        return []

    recommended_songs = [
        {
            "song_title": combined_dfs_cleaned.iloc[song_index]['song_title'],
            "similarity_score": similarity_score
        }
        for song_index, similarity_score in zip(song_indices[:top_k], similarity_scores[:top_k])
    ]

    return recommended_songs

# Example usage
book_title = input("Enter a book title: ")
recommended_songs = recommend_songs(book_title, top_k=5)

if recommended_songs:
    print("Top recommended songs:")
    for song in recommended_songs:
        print(f"- {song['song_title']}(Similarity: {song['similarity_score']:.2f})")
else:
    print("No recommendations found.")